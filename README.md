# Take_Home_Assessment

Approach:-

1. Created a base scrapy spider template.
2. Inspected the provided website base URL, and interpreted the results that it's a dynamically loading webpage. 
3. Thus, fetched the background APIs from the network tab. Basically, the data serving APIs (JSON structured data).
4. From the JSON data of the main page, scraped the subcategories data from categories data. And stored the required key such as subcategories_code, number of products present, etc. 
5. Passed that code and other arguments to the subcategories API URL, as arguments.
6. From the response generated by the new URLs, select or filter out the requirements i.e. the keys that are required in the result, to store them in the database.
7. After completing the functions, and the whole workflow. Created a temporary data storing object (scrapy fields from items) - to store the resultant keys. 
8. With the help of scrapy's pipeline method, set up a flow to pass the result container to the MongoDB database.
9. Run the scrapy spider to completely scrape the data and store it successively to the database.


Folder Structure:-

1. In the folder learning_scrapy there are 

Spiders Folder: This folder contains all of our future Scrapy spider files that extract the data.
Items: This file contains item objects that behave like Python dictionaries and provide an abstraction layer to store scraped data within the Scrapy framework.
Middlewares : Middlewares are useful if you want to modify how Scrapy runs and makes requests to the server.
Pipelines: Scrapy Pipelines are for extra data processing steps you want to implement after you extract data. You can clean, organize, or even drop data in these pipelines.
Settings: General settings for how Scrapy runs, for example, delays between requests, caching, file download settings, etc.

2. In the folder Screenshorts there are screenshots of the terminal

3. In the folder database there are two files one is database in csv in CSV format and another one is database in json in JSON format
